YOLOv8 Space Station Object Detection
Detecting FireExtinguisher, ToolBox, and OxygenTank in Synthetic Space Station Images


![Space Station Detection](image1.png)
![Space Station Detection](image2.png)

ğŸš€ Overview
This project leverages the YOLOv8 framework to detect FireExtinguisher, ToolBox, and OxygenTank in Falcon-generated synthetic space station imagery. The pipelineâ€”from environment setup through model training and result evaluationâ€”is fully reproducible.

ğŸ“¦ Project Access
Full Project Download
To access all scripts, configuration files, trained models, outputs, and assets, download the complete project folder from Google Drive:

ğŸ‘‰ Download the full project folder here
https://drive.google.com/file/d/1f74YSBu_m947X2nvjkN1VqK32i_8V4k-/view?usp=sharing

Note:
The GitHub repository contains only the code (scripts/configs) for convenience and versioning. All large files, datasets, training outputs, and assets are on Google Drive due to size constraints.

ğŸ“š Environment & Dependency Setup
Install Anaconda (Recommended)

Download and install from anaconda.com/download if not already present.

Set Up the Python Environment

bash
cd path\to\Hackathon_Dataset\ENV_SETUP
setup_env.bat
conda activate EDU
This creates and activates the EDU environment with Python 3.8+, PyTorch (CUDA if available), Ultralytics YOLOv8 (â‰¥8.3.171), OpenCV, numpy, PyYAML, etc.

ğŸ“‚ Download & Organize the Dataset
You do not need to download the dataset separatelyâ€”it is included in the full project Google Drive folder.
After downloading and extracting, your folder should look like this:

text
Hackathon_Dataset/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train/
â”‚ â”‚ â”œâ”€â”€ images/
â”‚ â”‚ â””â”€â”€ labels/
â”‚ â”œâ”€â”€ val/
â”‚ â”‚ â”œâ”€â”€ images/
â”‚ â”‚ â””â”€â”€ labels/
â”‚ â””â”€â”€ test/
â”‚ â”œâ”€â”€ images/
â”‚ â””â”€â”€ labels/
â”œâ”€â”€ yolo_params.yaml
â”œâ”€â”€ train.py
â”œâ”€â”€ predict.py
â””â”€â”€ visualize.py
Each image in images/ has a .txt label in labels/, formatted with normalized YOLO bounding boxes.

ğŸ“„ Configuration
Ensure your yolo_params.yaml looks like:

text
train: data/train/images
val: data/val/images
test: data/test/images
nc: 3
names: ["FireExtinguisher", "ToolBox", "OxygenTank"]
To add extra training folders, include them as a YAML list.

ğŸ”§ Quick Start: Run & Test the Model
Activate the Environment

bash
conda activate EDU
Train the YOLOv8 Model

bash
python train.py --epochs 25 --mosaic 0.1 --optimizer AdamW --momentum 0.2 --lr0 0.001 --lrf 0.0001
Reads dataset/config paths from yolo_params.yaml.

Predict/Evaluate on Test Data

bash
python predict.py
Outputs predictions to predictions/images/ and evaluation metrics to the console.

(Optional) Visualize Labeled Data

bash
python visualize.py
Use A and D to scroll through labeled samples.

â™»ï¸ Reproducing Results
Follow steps above for environment and folder structure.

Typical expected test metrics:

mAP@0.5 â‰ˆ 0.84

Precision â‰ˆ 0.91

Recall â‰ˆ 0.75

Output images in predictions/images/.

ğŸ“ˆ Output Interpretation
Training Folders:
runs/detect/ contains logs, weights (best.pt), plots (loss curves, metrics).

Prediction Folder:
predictions/images/ holds test images with bounding boxes and confidence scores.

Metrics:

Precision: Fraction of detections that are correct.

Recall: Fraction of all true objects detected.

mAP@0.5: "Standard" detection accuracy.

mAP@0.5â€“0.95: Bounding box localization quality.

ğŸ›  Troubleshooting
All images/ files must have matching .txt labels in labels/.

Confirm all folder paths are correct and relative.

Always activate the EDU environment before running code.

For large files, ensure you have enough local disk space after Google Drive extraction.

ğŸ“š References
Ultralytics YOLOv8 Documentation

YOLOv8 Annotation Format
